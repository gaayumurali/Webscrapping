# -*- coding: utf-8 -*-
"""r.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Md5MMeYaic3WfUj93EbinhIL3uCg9B3q
"""

#package for scraping
!pip install bs4

from bs4 import BeautifulSoup as bs
#sending request to access the content
import requests
#writing it in csv format
from csv import writer 
import pandas as pd

links = ['https://data.unicef.org/resources/wash-in-schools/',
        'https://www.right-to-education.org/monitoring/content/percentage-schools-potable-water']
with open('schooldrinkingwater.csv','w',encoding='utf8',newline='') as f:
  thewriter=writer(f)   
  header=['websites'] 
  thewriter.writerow(header)  
  for link in links:
    response = requests.get(link)
    html = response.content
    soup = bs(html,'html.parser')
    paragraphs = soup.find_all('p')
    for p in paragraphs:
      para = p.text
      info=[para]
      thewriter.writerow(info)

df=pd.read_csv('/content/schooldrinkingwater.csv')
df

df=df.dropna(how='all')
df

import nltk                               
import matplotlib.pyplot as plt           
import random  
import pandas as pd


#Removing punctuation
import string
string.punctuation
#Defining Function to remove punctuation
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree
#Storing the puntuation free text
df['clean']= df['websites'].apply(lambda x:remove_punctuation(x))
display(df.head(3))
#Lowering the tweets
df['lower']= df['clean'].apply(lambda x: x.lower())

#Tokenization
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TweetTokenizer as tt
#applying function to the column
tokenizer = tt()      
df['tokenized'] = df['lower'].apply(lambda x: tokenizer.tokenize(x))

#Removing stop words
from nltk.corpus import stopwords
nltk.download('stopwords')
stopword = stopwords.words('english')
#Defining the function to remove stopwords from tokenized text
def remove_stopwords(text):
    output= [i for i in text if i not in stopword]
    return output
#applying the function
df['stopwords_remove']= df['tokenized'].apply(lambda x:remove_stopwords(x))

#Stemming 
from nltk.stem.porter import PorterStemmer
porter_stemmer = PorterStemmer()
#Defining a function for stemming
def stemming(text):
    stem_text = [porter_stemmer.stem(word) for word in text]
    return stem_text
df['stemmed']=df['stopwords_remove'].apply(lambda x: stemming(x))

#Lemmatization
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')
wordnet_lemmatizer = WordNetLemmatizer()
#Defining the function for lemmatization
def lemmatizer(text):
    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]
    return lemm_text
df['lemmatized']=df['stemmed'].apply(lambda x:lemmatizer(x))
df.head(5)